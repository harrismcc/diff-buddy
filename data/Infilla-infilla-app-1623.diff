diff --git a/AGENTS.md b/AGENTS.md
index a485d612e..7d696ed48 100644
--- a/AGENTS.md
+++ b/AGENTS.md
@@ -131,7 +131,6 @@ import { MyComponent } from './my-component';
 npm run dev              # Start dev server
 npm run dev:docker       # Start Docker services
 npm run prisma:migrate:local  # Run DB migrations
-npm run opensearch:migrate    # Run search migrations
 ```
 
 ### Code Quality (run in parallel after changes)
diff --git a/Dockerfile-app-server b/Dockerfile-app-server
index 638c65148..2d96ac337 100644
--- a/Dockerfile-app-server
+++ b/Dockerfile-app-server
@@ -36,7 +36,6 @@ WORKDIR /usr/server/app
 COPY ./app ./app
 COPY ./modules ./modules
 COPY ./data ./data
-COPY ./opensearch ./opensearch
 COPY ./packages ./packages
 COPY ./public ./public
 COPY ./types ./types
diff --git a/README.md b/README.md
index 967255f72..b673245df 100644
--- a/README.md
+++ b/README.md
@@ -57,12 +57,15 @@ The fastest way to get started is using [Dev Containers](https://containers.dev/
 3. Open in your IDE and start the Dev Container:
 
    **For VS Code (recommended):**
+
    ```sh
    code .
    ```
+
    When prompted, click **"Reopen in Container"** (or use Command Palette: `Dev Containers: Reopen in Container`)
 
    **For other IDEs:**
+
    - Most IDEs have a similar "Reopen in Container" option in their command palette
    - For JetBrains IDEs, you can use the [Dev Container integration](https://www.jetbrains.com/help/idea/connect-to-devcontainer.html) or the [Dev Container CLI](https://jetbrains.com/help/phpstorm/dev-container-cli.html)
    - For Cursor, use the built-in Dev Container support similar to VS Code
@@ -73,13 +76,12 @@ The fastest way to get started is using [Dev Containers](https://containers.dev/
    - Install Node.js 22.13.1
    - Install npm dependencies
    - Generate Prisma client
-   - Start backend services (PostgreSQL, OpenSearch, LocalStack, etc.)
+   - Start backend services (PostgreSQL, LocalStack, etc.)
 
 4. Run database migrations:
 
    ```sh
    npm run prisma:migrate:local
-   npm run opensearch:migrate
    ```
 
 5. Start the development server:
@@ -155,7 +157,6 @@ If you prefer not to use Dev Containers, you can set up the environment manually
    ```sh
    npm run prisma:generate
    npm run prisma:migrate:local
-   npm run opensearch:migrate
    ```
 
    **Troubleshooting PostgreSQL access errors:**
@@ -220,5 +221,3 @@ In the admin panel, look for and enable these toggles:
 Although `infilla-app` is designed to run on a single laptop for development, you need to deploy to AWS to make it available for the rest of the web.
 
 Read the [Infrastructure](./docs/infrastructure.md) guide for details on how to do this
-
-
diff --git a/WARP.md b/WARP.md
index 9754aead4..374fcfdac 100644
--- a/WARP.md
+++ b/WARP.md
@@ -5,17 +5,17 @@ This file provides guidance to WARP (warp.dev) when working with code in this re
 ## Development Commands
 
 ### Local Development Setup
+
 ```bash
 # Install dependencies
 npm install
 
-# Start local Docker services (Postgres, OpenSearch, LocalStack)
+# Start local Docker services (Postgres, LocalStack)
 npm run dev:docker
 
 # In a new terminal, run migrations to set up the schema
 npm run prisma:generate
 npm run prisma:migrate:local
-npm run opensearch:migrate
 
 # Run the development server
 npm run dev
@@ -25,7 +25,8 @@ npm run dev
 ```
 
 ### Development Workflow
-```bash
+
+````bash
 # Development server variants
 npm run dev                    # Standard dev mode on port 3000
 npm run dev:public            # Dev mode accessible from network (0.0.0.0)
@@ -51,16 +52,14 @@ npm run prettier:fix         # Fix formatting on changed files
 npm run prettier:fix:all     # Fix all formatting issues
 npm run typecheck            # Run TypeScript type checking
 
-# OpenSearch operations
-npm run opensearch:reindex    # Reindex OpenSearch data
-```
-
 ### Authentication for Development
+
 - Use fake login endpoint: `/auth/fake-login?email=<seeded-user-email>&redirectTo=<path>`
 - Database must be seeded with valid user
 - Example paths: `/forum`, `/plan-check`
 
 ### Deployment Commands
+
 ```bash
 # CDK operations
 npm run cdk:diff             # Show infrastructure changes
@@ -70,16 +69,16 @@ npm run cdk:deploy-production # Deploy to production AWS environment
 # Build operations
 npm run build                # Build for development mode
 npm run build:docker         # Build Docker image
-```
+````
 
 ## Architecture Overview
 
 ### Tech Stack
+
 - **Framework**: Remix (React-based full-stack framework)
 - **Language**: TypeScript throughout (client and server)
 - **Database**: PostgreSQL with Prisma ORM
 - **UI Components**: Radix UI Themes for styled components
-- **Search**: OpenSearch for document search capabilities
 - **Build Tool**: Vite for fast development and builds
 - **Testing**: Jest with React Testing Library
 - **Deployment**: AWS CDK for Infrastructure as Code
@@ -88,6 +87,7 @@ npm run build:docker         # Build Docker image
 ### Key Directories
 
 - **`app/`**: Core Remix application
+
   - `routes/`: File-based routing with flat routes structure
     - `_app.*`: Main application routes (requires auth)
     - `_admin.*`: Admin-only routes
@@ -101,6 +101,7 @@ npm run build:docker         # Build Docker image
   - `emails/`: Email template components
 
 - **`data/`**: Application configuration data
+
   - `forms/`: New dynamic forms system configurations (JSON Schema-based)
 
 - **`prisma/`**: Database schema and migrations
@@ -109,9 +110,11 @@ npm run build:docker         # Build Docker image
 - **`docs/`**: Project documentation
 
 ### Form Systems Architecture
+
 The application has two distinct form systems:
 
 1. **New Dynamic Forms System** (`_forms.*` routes, `data/forms/`):
+
    - Uses JSON Schema for declarative form configuration
    - Supports complex validation and conditional logic
    - Integrates with Accela for government workflow systems
@@ -122,6 +125,7 @@ The application has two distinct form systems:
    - Not used for new development
 
 ### Multi-tenant Architecture
+
 - All customers share the same database
 - `Organization` entity and `organizationId` properties provide tenant isolation
 - Jurisdiction-specific configuration managed in code (in `data/` folder)
@@ -129,17 +133,20 @@ The application has two distinct form systems:
 ### Key Patterns
 
 #### Authentication & Authorization
+
 - OAuth-only authentication (Google, Microsoft Entra)
 - Role-based access control with organization-level permissions
 - Multi-tenant data isolation via `organizationId`
 
 #### Data Flow
+
 - Remix loaders for server-side data fetching
 - Remix actions for data mutations
 - Prisma for type-safe database operations
 - Feed-forward (unidirectional) data flows preferred
 
 #### Component Architecture
+
 - Named function exports for React components
 - Radix UI components preferred over HTML elements
 - SCSS modules for styling when needed
@@ -148,6 +155,7 @@ The application has two distinct form systems:
 ## Development Guidelines
 
 ### Code Style
+
 - Use kebab-case for file names
 - Use camelCase for variables (first letter of each word capitalized: PdfViewer, SupportsIos)
 - Named exports for components, loaders, and actions
@@ -155,26 +163,31 @@ The application has two distinct form systems:
 - Leverage type safety to prevent API misuse
 
 ### React Conventions
+
 - Default to Radix components (`Box` instead of `div`)
 - Named functions for components: `export function MyComponent() {}`
 - High contrast UI elements - prefer built-in themes over custom colors
 - Styling preference: Radix styles → inline styles → `.module.scss` files
 
 ### Testing Approach
+
 - Behavioral testing focused on user-visible behavior
 - Progressive complexity: unit tests preferred over integration tests
 - Test-driven development: write failing test first, then implementation
 - Tests should be robust to implementation refactors
 
 ### Import Organization
+
 Organize imports in this order:
+
 1. React imports
-2. Remix imports  
+2. Remix imports
 3. Third-party libraries
 4. App-specific modules
 5. Relative imports
 
 ### Performance Considerations
+
 - The app uses Docker for local development to ensure offline capability
 - LocalStack provides AWS service mocks for development
 - Remix provides optimized client/server boundaries
@@ -182,11 +195,13 @@ Organize imports in this order:
 ## Common Development Tasks
 
 ### Running a Single Test
+
 ```bash
 npm run test -- path/to/test.tsx
 ```
 
 ### Database Schema Changes
+
 ```bash
 # Create and apply migration
 npm run prisma:migrate:local
@@ -196,33 +211,38 @@ npm run prisma:generate
 ```
 
 ### Adding New Routes
+
 - Use flat routes structure in `app/routes/`
 - Follow naming conventions: `_app.feature.action.tsx`
 - Include route group prefixes for proper layout nesting
 
 ### Quality Checks Before Committing
+
 Always run these commands in parallel after making changes:
+
 ```bash
 npm run typecheck && npm run lint:fix && npm run prettier:fix && npm run test
 ```
 
 ## Environment Configuration
+
 - Development uses `.env.development` with Docker services
 - Production/staging environments use AWS Secrets Manager
 - Node.js version: 22.13.1 (managed via nvm)
 - Git-crypt used for encrypted environment files
 
 ## Deployment Architecture
+
 - AWS multi-account strategy (development/production isolation)
 - Infrastructure as Code using AWS CDK
 - Container-based deployment with Docker
 - CloudFront + ALB for web serving
 - AWS RDS (PostgreSQL) for database
-- AWS OpenSearch for search functionality
 - Sentry for error monitoring
 - PostHog for product analytics
 
 ## Important Development Notes
+
 - The codebase prioritizes junior developer accessibility
 - Documentation should be kept evergreen and updated with changes
 - Clean up code before committing (unused variables, consistent naming, etc.)
diff --git a/app/queries/thread-list.server.ts b/app/queries/thread-list.server.ts
index dc9d24934..7b4a4eb8c 100644
--- a/app/queries/thread-list.server.ts
+++ b/app/queries/thread-list.server.ts
@@ -13,7 +13,6 @@ import { AuthenticatedUser } from '../services/authentication/forum-auth.server'
 import { getPrisma } from '../services/db.server';
 import { captureServerError } from '../services/sentry.server';
 import { getThreadSpiceClient } from '../services/spice.server';
-import { formatRegulationId } from '../utils/regulation';
 import { type ThreadListFilterState } from '../utils/thread-list-filter-state';
 
 import { buildThreadSearchWhereClause } from './helpers/buildThreadSearchWhereClause.server';
@@ -296,221 +295,6 @@ export const searchThreads = async <ThreadIncludeT extends Prisma.ThreadInclude>
   }
 };
 
-const _toOpenSearchQuery = ({
-  filters,
-  organizationId,
-  isPublished,
-  threadIds,
-}: {
-  filters: ThreadListFilterState;
-  organizationId: string;
-  isPublished?: boolean;
-  threadIds?: string[];
-}) => {
-  // TODO: stricter typing
-  const andClauses: object[] = [{ term: { organizationId } }];
-
-  if (typeof isPublished === 'boolean') {
-    andClauses.push({
-      term: { isPublished },
-    });
-  }
-
-  if (threadIds) {
-    andClauses.push({
-      terms: { id: threadIds },
-    });
-  }
-
-  if (filters.status.length) {
-    andClauses.push({
-      terms: { status: filters.status },
-    });
-  }
-
-  if (filters.topicTagSlugs.length) {
-    const searchingForTagless = filters.topicTagSlugs.includes(NO_CATEGORY_TOPIC_TAG.slug);
-    const searchForTags = !searchingForTaglessOnly(filters.topicTagSlugs);
-
-    const topicTagSubQuery: { bool: { should: object[] } } = {
-      bool: {
-        should: [],
-      },
-    };
-
-    if (searchForTags) {
-      topicTagSubQuery.bool.should.push({
-        terms: { topicTagSlugs: filters.topicTagSlugs },
-      });
-    }
-
-    if (searchingForTagless) {
-      topicTagSubQuery.bool.should.push({
-        bool: {
-          must_not: [
-            {
-              exists: {
-                field: 'topicTagSlugs',
-              },
-            },
-          ],
-        },
-      });
-    }
-
-    andClauses.push(topicTagSubQuery);
-  }
-
-  if (filters.regulationIds.length) {
-    andClauses.push({
-      terms: { legalCodeIds: filters.regulationIds.map(formatRegulationId) },
-    });
-  }
-
-  if (filters.userIds.length) {
-    // Check if we're filtering for external users (special 'external' sentinel value)
-    const hasExternalUserFilter = filters.userIds.includes(EXTERNAL_USER_STUB.id);
-    const regularUserIds = filters.userIds.filter((id) => id !== EXTERNAL_USER_STUB.id);
-
-    const userFilterConditions: object[] = [];
-
-    // Add regular user IDs if any
-    if (regularUserIds.length > 0) {
-      userFilterConditions.push({
-        terms: {
-          threadAuthorId: regularUserIds,
-        },
-      });
-    }
-
-    // Add external user filter if selected (threadAuthorId is empty string and viaChannel is 'public')
-    if (hasExternalUserFilter) {
-      userFilterConditions.push({
-        bool: {
-          must: [
-            {
-              term: {
-                threadAuthorId: '',
-              },
-            },
-            {
-              term: {
-                viaChannel: 'public',
-              },
-            },
-          ],
-        },
-      });
-    }
-
-    // Combine conditions with OR if we have both
-    if (userFilterConditions.length > 0) {
-      andClauses.push({
-        bool: {
-          should: userFilterConditions,
-        },
-      });
-    }
-  }
-
-  if (filters.questionTypes.length) {
-    // If 'Question' is selected, include both 'Question' and null/undefined questionType
-    const includesQuestion = filters.questionTypes.includes('Question');
-    const otherQuestionTypes = filters.questionTypes.filter((type) => type !== 'Question');
-
-    const questionTypeConditions: object[] = [];
-
-    // Add conditions for specific question types (excluding 'Question')
-    if (otherQuestionTypes.length > 0) {
-      questionTypeConditions.push({
-        terms: { questionType: otherQuestionTypes },
-      });
-    }
-
-    // If 'Question' is selected, add condition for both 'Question' and null/undefined
-    if (includesQuestion) {
-      questionTypeConditions.push({
-        bool: {
-          should: [
-            { term: { questionType: 'Question' } },
-            { bool: { must_not: [{ exists: { field: 'questionType' } }] } },
-          ],
-        },
-      });
-    }
-
-    // Combine all conditions with OR
-    if (questionTypeConditions.length > 0) {
-      andClauses.push({
-        bool: {
-          should: questionTypeConditions,
-        },
-      });
-    }
-  }
-
-  // text query
-  andClauses.push({
-    bool: {
-      should: [
-        // exact token & stemming fields
-        {
-          multi_match: {
-            query: filters.q,
-            fields: [
-              'subject',
-              'subject.english',
-              'commentsBodyText',
-              'commentsBodyText.english',
-              'propertyAddress',
-              'propertyAddress.english',
-              'topicTagNames',
-              'topicTagNames.english',
-              'legalCodeTitles',
-              'legalCodeTitles.english',
-              'legalCodeShortTitles',
-            ],
-          },
-        },
-        // type-as-you-search fields
-        {
-          multi_match: {
-            query: filters.q,
-            type: 'bool_prefix',
-            fields: [
-              'subject.suggestions',
-              'subject.suggestions._2gram',
-              'subject.suggestions._3gram',
-              'subject.suggestions._index_prefix',
-              'propertyAddress.suggestions',
-              'propertyAddress.suggestions._2gram',
-              'propertyAddress.suggestions._3gram',
-              'propertyAddress.suggestions._index_prefix',
-            ],
-          },
-        },
-      ],
-    },
-  });
-
-  // Add sorting if provided
-  const sort: object[] = [];
-  const sortField = filters.getSortField();
-  const sortDirection = filters.getNextSortDirection();
-  if (sortField && sortDirection) {
-    sort.push({
-      [sortField]: { order: sortDirection || 'desc' },
-    });
-  }
-
-  return {
-    query: { bool: { must: andClauses } },
-    from: filters.toPrismaSkip(),
-    size: filters.toPrismaTake(),
-    sort,
-  };
-};
-
 const toPrismaQuery = ({
   filters,
   organizationId,
diff --git a/app/search/indexers/thread.server.ts b/app/search/indexers/thread.server.ts
index 64eaf4887..e8521098c 100644
--- a/app/search/indexers/thread.server.ts
+++ b/app/search/indexers/thread.server.ts
@@ -1,12 +1,9 @@
 import { type Prisma } from '@prisma/client';
 import * as Sentry from '@sentry/node';
 
-import { threadIndexSchema } from '../../../opensearch/schema';
-import { ResponseError } from '../../../opensearch/types';
 import { getPrisma } from '../../services/db.server';
 import logger from '../../services/logger.server';
 import { capture } from '../../services/posthog.server';
-import { getSearchClient } from '../../services/search.server';
 /**
  * Index a thread in the background
  *
@@ -20,14 +17,16 @@ export const indexThreadInBackground = (threadId: string) => {
     userId: 'backend',
     properties: { threadId },
   });
-  let success = false;
-  indexManyThreads({ where: { id: threadId } })
+  let success = true;
+
+  // No-op: Just resolve immediately without doing any indexing
+  Promise.resolve(0)
     .then((count) => {
-      success = true;
-      logger.info(`Indexed ${count} threads in ${Date.now() - start}ms`);
+      logger.info(`Skipped indexing ${count} threads in ${Date.now() - start}ms`);
     })
     .catch((e) => {
-      logger.error('Could not index opensearch threads', e);
+      success = false;
+      logger.error('Error in thread indexing no-op', e);
       Sentry.captureException(e, { level: 'error', extra: { threadId } });
     })
     .finally(() => {
@@ -51,7 +50,6 @@ export const indexManyThreads = async ({
   where: Prisma.ThreadWhereInput;
 }): Promise<number> => {
   const prisma = await getPrisma();
-  const searchClient = await getSearchClient();
 
   const threads = await prisma.thread.findMany({
     where,
@@ -65,42 +63,6 @@ export const indexManyThreads = async ({
   });
   logger.info(`Fetched ${threads.length} threads`);
 
-  let indexedDocsCount = 0;
-  for (const thread of threads) {
-    // log every 100 indexed threads
-    if (indexedDocsCount % 100 == 0) {
-      const completionPerc = Math.round((indexedDocsCount * 100) / threads.length);
-      logger.info(`Indexed ${indexedDocsCount} (${completionPerc}% complete)`);
-    }
-    const doc = threadIndexSchema.transform(thread);
-
-    if (doc) {
-      await searchClient.index({
-        id: thread.id,
-        index: threadIndexSchema.name,
-        body: doc,
-        refresh: true, // what does this do?
-      });
-    } else {
-      try {
-        await searchClient.delete({
-          id: thread.id,
-          index: threadIndexSchema.name,
-        });
-      } catch (e) {
-        if ((e as Error).name != 'ResponseError') {
-          throw e;
-        }
-        const error = e as ResponseError;
-        // continue if already deleted
-        if (error.meta.statusCode != 404) {
-          throw e;
-        }
-      }
-    }
-
-    indexedDocsCount += 1;
-  }
-
-  return indexedDocsCount;
+  // No-op: Return count of threads but don't actually index them
+  return threads.length;
 };
diff --git a/app/services/email.server.integration.test.tsx b/app/services/email.server.integration.test.tsx
index e1e2ad6ef..03edd637d 100644
--- a/app/services/email.server.integration.test.tsx
+++ b/app/services/email.server.integration.test.tsx
@@ -20,12 +20,6 @@ import {
   _getNewQuestionEmailDetails,
 } from './email.server';
 
-// Mock the indexThreadInBackground function to prevent background processes in tests
-/** @TODO move this into base when we can figure out how to get around opensearch dependency in tests */
-jest.mock('../search/indexers/thread.server', () => ({
-  indexThreadInBackground: jest.fn(),
-}));
-
 beforeEach(async () => {
   await resetDb();
 });
diff --git a/app/services/search.server.ts b/app/services/search.server.ts
deleted file mode 100644
index 67eeaa675..000000000
--- a/app/services/search.server.ts
+++ /dev/null
@@ -1,69 +0,0 @@
-import AWSXRay from 'aws-xray-sdk-core';
-
-import { Client as SearchClient } from '@opensearch-project/opensearch';
-
-import { asyncSingleton } from '../utils/singleton.server';
-
-export { type SearchClient };
-
-export const getSearchClient = async (): Promise<SearchClient> => {
-  return asyncSingleton('searchClient', async () => {
-    if (!process.env.OPENSEARCH_URL) {
-      throw new Error('Missing OPENSEARCH_URL');
-    }
-
-    const client = new SearchClient({
-      node: process.env.OPENSEARCH_URL,
-    });
-
-    const requestStartTimeMsById = new Map<number, number>();
-
-    client.on('request', (_err, request) => {
-      requestStartTimeMsById.set(request.meta.request.id, Date.now());
-    });
-
-    client.on('response', (_err, request) => {
-      try {
-        const startTimeMs = requestStartTimeMsById.get(request.meta.request.id);
-        const endTimeMs = Date.now();
-        requestStartTimeMsById.delete(request.meta.request.id);
-        if (typeof startTimeMs != 'number') {
-          throw new Error(`Missing start time for ${request.meta.request.id}`);
-        }
-
-        // Report to X-Ray if this request has tracing enabled
-        // https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html#configuration-envvars-runtime
-        if (process.env._X_AMZN_TRACE_ID) {
-          const segment = AWSXRay.getSegment();
-          if (segment) {
-            // Add a new Subsegment to parent Segment
-            const subSegment = segment.addNewSubsegment('OpenSearch');
-            // Add data to the segment
-            // (names from https://opentelemetry.io/docs/specs/semconv/http/http-spans/)
-            subSegment.addMetadata('http.request.method', request.meta.request.params.method);
-            subSegment.addMetadata('http.response.status_code', request.statusCode);
-            subSegment.addMetadata('url.path', request.meta.request.params.path);
-            subSegment.addMetadata('server.address', request.meta.connection.url.hostname);
-            subSegment.addMetadata('server.port', request.meta.connection.url.port);
-            subSegment.addMetadata(
-              'network.protocol.name',
-              request.meta.connection.url.protocol.replace(':', ''),
-            );
-            // X-Ray wants the time in seconds -> ms
-            subSegment.addAttribute('start_time', startTimeMs / 1_000);
-            subSegment.addAttribute('end_time', endTimeMs / 1_000);
-            // Set in_progress to false so subSegment
-            // will be sent to xray on streamSubsegments()
-            subSegment.addAttribute('in_progress', false);
-            subSegment.streamSubsegments();
-          }
-        }
-      } catch (e) {
-        // eslint-disable-next-line no-console
-        console.error(e);
-      }
-    });
-
-    return client;
-  });
-};
diff --git a/cdk.json b/cdk.json
index 8e9e2e19a..1b81e8252 100644
--- a/cdk.json
+++ b/cdk.json
@@ -48,7 +48,6 @@
     "@aws-cdk/aws-autoscaling:generateLaunchTemplateInsteadOfLaunchConfig": true,
     "@aws-cdk/core:includePrefixInUniqueNameGeneration": true,
     "@aws-cdk/aws-efs:denyAnonymousAccess": true,
-    "@aws-cdk/aws-opensearchservice:enableOpensearchMultiAzWithStandby": true,
     "@aws-cdk/aws-lambda-nodejs:useLatestRuntimeVersion": true,
     "@aws-cdk/aws-efs:mountTargetOrderInsensitiveLogicalId": true,
     "@aws-cdk/aws-rds:auroraClusterChangeScopeOfInstanceParameterGroupWithEachParameters": true,
diff --git a/deployment/lib/constructs/InfillaApplication.ts b/deployment/lib/constructs/InfillaApplication.ts
index e06e1d081..db8ec1f31 100644
--- a/deployment/lib/constructs/InfillaApplication.ts
+++ b/deployment/lib/constructs/InfillaApplication.ts
@@ -15,7 +15,6 @@ import { FargateServiceDeployment } from './remix/FargateServiceDeployment';
 import { FileStorage } from './FileStorage';
 import { MigrationRunner } from './MigrationRunner';
 import { ScheduledLambda } from './ScheduledLambda';
-import { type SearchDomain } from './SearchDomain';
 
 export interface InfillaApplicationProps {
   mode: string;
@@ -25,8 +24,6 @@ export interface InfillaApplicationProps {
   dbSecret: ISecret;
   /** Where the app should upload files to */
   storage: FileStorage;
-  /** The app's OpenSearch cluster */
-  searchDomain: SearchDomain;
   /** Root URL of the app */
   rootUrl: string;
   /** Domain name the app is hosted with */
@@ -52,7 +49,6 @@ export interface InfillaApplicationProps {
 
 const REMIX_SERVICE_LOG_GROUP_NAME = 'infilla-remix-service';
 const DB_MIGRATION_RUNNER_LOG_GROUP_NAME = 'infilla-db-migration';
-const SEARCH_MIGRATION_RUNNER_LOG_GROUP_NAME = 'infilla-search-migration';
 const ACCELA_TOKEN_REFRESHER_LOG_GROUP_NAME = 'infilla-accela-token-refresher';
 const DAILY_DIGEST_EMAILS_LOG_GROUP_NAME = 'infilla-daily-digest-emails';
 // TODO: read these from the actual build folder instead of hardcoding
@@ -90,8 +86,6 @@ export class InfillaApplication extends Construct {
 
     const stackName = Stack.of(this).stackName;
 
-    const searchUrl = `https://${props.searchDomain.endpoint()}`;
-
     this.appSecurityGroup = new SecurityGroup(this, 'SecurityGroup', {
       vpc: props.vpc,
     });
@@ -122,7 +116,6 @@ export class InfillaApplication extends Construct {
         SENTRY_RELEASE: props.sentry.release,
         SES_REGION: props.sesRegion,
         STORAGE_BUCKET: props.storage.bucketName(),
-        OPENSEARCH_URL: searchUrl,
         RADAR_CLIENT_KEY: props.radar.clientKey,
         KMS_KEY_ALIAS,
         ...requireEnv('WORKFLOWAI_API_KEY'),
@@ -163,22 +156,6 @@ export class InfillaApplication extends Construct {
       logGroupName: `/aws/lambda/${stackName}/${DB_MIGRATION_RUNNER_LOG_GROUP_NAME}`,
     });
 
-    // Search migration runner
-    this.searchMigrationRunner = new MigrationRunner(this, 'SearchMigration', {
-      description: 'Search migration runner, managed by CDK',
-      entry: 'deployment/lib/lambdas/opensearch-command-runner.ts',
-      environment: {
-        NODE_ENV: 'production',
-        DATABASE_SECRET_ARN: props.dbSecret.secretArn,
-        OPENSEARCH_URL: searchUrl,
-        POSTHOG_KEY: props.posthog.key,
-        POSTHOG_HOST: props.posthog.host,
-      },
-      vpc: props.vpc,
-      securityGroups: [this.appSecurityGroup],
-      logGroupName: `/aws/lambda/${stackName}/${SEARCH_MIGRATION_RUNNER_LOG_GROUP_NAME}`,
-    });
-
     // Accela token refresher
     this.accelaTokenRefresher = new ScheduledLambda(this, 'AccelaTokenRefresher', {
       vpc: props.vpc,
@@ -213,7 +190,6 @@ export class InfillaApplication extends Construct {
         SES_REGION: props.sesRegion,
         EMAIL_NOTIFICATIONS_ADDRESS: props.applicationEmails.notifications,
         STORAGE_BUCKET: props.storage.bucketName(),
-        OPENSEARCH_URL: searchUrl,
         RADAR_CLIENT_KEY: props.radar.clientKey,
         ...requireEnv('POSTMARK_SERVER_API_TOKEN'),
       },
@@ -242,10 +218,6 @@ export class InfillaApplication extends Construct {
     // Give application access to file storage
     props.storage.grantUploadDownloadAccessTo(this.remixService.taskRole());
 
-    // Give application access to search cluster
-    props.searchDomain.grantReadWrite(this.remixService.taskRole());
-    props.searchDomain.grantReadWrite(this.searchMigrationRunner.lambda());
-
     // Give application access to email sending
     props.dkimIdentity.grantSendEmail(this.remixService.taskRole());
     props.dkimIdentity.grantSendEmail(this.dailyDigestEmails.lambda());
diff --git a/deployment/lib/constructs/SearchDomain.ts b/deployment/lib/constructs/SearchDomain.ts
deleted file mode 100644
index 100c70b0c..000000000
--- a/deployment/lib/constructs/SearchDomain.ts
+++ /dev/null
@@ -1,73 +0,0 @@
-import { ISecurityGroup, IVpc, Port, SubnetType } from 'aws-cdk-lib/aws-ec2';
-import { AnyPrincipal, type IGrantable, PolicyStatement } from 'aws-cdk-lib/aws-iam';
-import { Domain, EngineVersion, TLSSecurityPolicy } from 'aws-cdk-lib/aws-opensearchservice';
-import { Construct } from 'constructs';
-
-export interface SearchDomainProps {
-  vpc: IVpc;
-}
-
-/**
- * OpenSearch cluster
- *
- * TODO: Migrate to a serverless cluster once L2 constructs are available (https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_opensearchserverless-readme.html#awsopensearchserverless-construct-library)
- */
-export class SearchDomain extends Construct {
-  private readonly domain: Domain;
-
-  constructor(scope: Construct, id: string, props: SearchDomainProps) {
-    super(scope, id);
-
-    this.domain = new Domain(this, 'Domain', {
-      version: EngineVersion.openSearch('2.19'),
-      enableVersionUpgrade: true,
-      vpc: props.vpc,
-      // networking config from https://github.com/aws/aws-cdk/issues/29346
-      vpcSubnets: [
-        {
-          subnets: [
-            props.vpc.selectSubnets({
-              subnetType: SubnetType.PRIVATE_WITH_EGRESS,
-            }).subnets[0],
-          ],
-        },
-      ],
-      // smallest servers we can get
-      capacity: {
-        dataNodeInstanceType: 't3.small.search',
-        dataNodes: 1,
-        multiAzWithStandbyEnabled: false,
-      },
-      // required for encryption at rest + in transit
-      encryptionAtRest: { enabled: true },
-      nodeToNodeEncryption: true,
-      enforceHttps: true,
-      tlsSecurityPolicy: TLSSecurityPolicy.TLS_1_2_PFS,
-    });
-
-    // Allow anonymous requests (still secure because domain is in a VPC)
-    // https://dev.to/aws-builders/access-control-and-opensearch-service-security-2lkn#Fine-grained-access-control
-    //
-    // TODO: narrower permissions (e.g. app services use basic auth or execution role to connect)
-    this.domain.addAccessPolicies(
-      new PolicyStatement({
-        principals: [new AnyPrincipal()],
-        actions: ['es:ESHttp*'],
-      }),
-    );
-  }
-
-  grantReadWrite(identity: IGrantable) {
-    this.domain.grantReadWrite(identity);
-  }
-
-  // For more on OpenSearch access control, see https://docs.aws.amazon.com/opensearch-service/latest/developerguide/fgac.html#fgac-access-policies
-  allowDefaultPortFrom(securityGroup: ISecurityGroup) {
-    // Allow connections via https
-    this.domain.connections.allowFrom(securityGroup, Port.HTTPS);
-  }
-
-  endpoint(): string {
-    return this.domain.domainEndpoint;
-  }
-}
diff --git a/deployment/lib/infilla-app-stack.ts b/deployment/lib/infilla-app-stack.ts
index 49b0b37a6..f707f42c5 100644
--- a/deployment/lib/infilla-app-stack.ts
+++ b/deployment/lib/infilla-app-stack.ts
@@ -22,7 +22,6 @@ import {
   STATIC_BUILD_FOLDERS,
 } from './constructs/InfillaApplication';
 import { RemixDistribution } from './constructs/remix/RemixDistribution';
-import { SearchDomain } from './constructs/SearchDomain';
 
 interface InfillaAppStackProps extends StackProps {
   /**
@@ -79,7 +78,6 @@ interface InfillaAppStackProps extends StackProps {
 export class InfillaAppStack extends Stack {
   private readonly db: DatabaseCluster;
   private readonly fileStorage: FileStorage;
-  private readonly searchDomain: SearchDomain;
 
   // eslint-disable-next-line @typescript-eslint/no-useless-constructor
   constructor(scope: Construct, id: string, props: InfillaAppStackProps) {
@@ -138,8 +136,6 @@ export class InfillaAppStack extends Stack {
       bucketName: props.storageBucketName,
     });
 
-    this.searchDomain = new SearchDomain(this, 'Search', { vpc });
-
     // Application layer
     // ========================================================================
     if (!props.env?.region) {
@@ -158,17 +154,14 @@ export class InfillaAppStack extends Stack {
       dbSecret: this.db.secret,
       storage: this.fileStorage,
       authSecret,
-      searchDomain: this.searchDomain,
       vpc,
       radar: props.radar,
     });
 
     this.db.connections.allowDefaultPortFrom(application.securityGroup());
-    this.searchDomain.allowDefaultPortFrom(application.securityGroup());
 
     // make sure migrations are executed after the database/search clusters are available
     application.dbMigrationTrigger().executeAfter(this.db);
-    application.searchMigrationTrigger().executeAfter(this.searchDomain);
 
     // Distribution layer
     // ========================================================================
diff --git a/deployment/lib/lambdas/opensearch-command-runner.ts b/deployment/lib/lambdas/opensearch-command-runner.ts
deleted file mode 100644
index 99ece19d3..000000000
--- a/deployment/lib/lambdas/opensearch-command-runner.ts
+++ /dev/null
@@ -1,25 +0,0 @@
-import { type Handler } from 'aws-lambda';
-
-import { runMigrate, runReindex } from '../../../scripts/opensearch/commands';
-
-interface HandlerEvent {
-  command?: string;
-}
-
-/**
- * Run commands from `./scripts/opensearch.ts`
- */
-export const handler: Handler = async (event: HandlerEvent, _) => {
-  const command = event.command ?? 'migrate';
-
-  switch (command) {
-    case 'migrate':
-      await runMigrate();
-      break;
-    case 'reindex':
-      await runReindex();
-      break;
-    default:
-      throw new Error(`Unknown command ${command}`);
-  }
-};
diff --git a/docker-compose.base.yml b/docker-compose.base.yml
index 48d8f4ba0..83570f17e 100644
--- a/docker-compose.base.yml
+++ b/docker-compose.base.yml
@@ -37,42 +37,6 @@ services:
       - KMS_ENDPOINT=${KMS_ENDPOINT}
       - S3_SKIP_SIGNATURE_VALIDATION=0 # allows easy S3 failure replication (tweak presigned URL by constant)
       - PERSISTENCE=1
-  # OpenSearch config modified from https://opensearch.org/docs/latest/install-and-configure/install-opensearch/docker/#sample-docker-compose-file-for-development
-  opensearch-node:
-    image: opensearchproject/opensearch:latest
-    container_name: opensearch-node
-    environment:
-      # Name the cluster
-      - cluster.name=opensearch-cluster
-      # Name the node that will run in this container
-      - node.name=opensearch-node
-      # Run single node as the cluster
-      - discovery.type=single-node
-      # Disable JVM heap memory swapping
-      - bootstrap.memory_lock=true
-      # Set min and max JVM heap sizes to at least 50% of system RAM
-      - 'OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m'
-      # Fix Java incompatibility with SVE for arm Mac on Sequoia 15+
-      - '_JAVA_OPTIONS=-XX:UseSVE=0'
-      # Prevents execution of bundled demo script which installs demo certificates and security configurations to OpenSearch
-      - 'DISABLE_INSTALL_DEMO_CONFIG=true'
-      # Disables Security plugin
-      - 'DISABLE_SECURITY_PLUGIN=true'
-      # Required for OpenSearch 2.12 and later
-      # - OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_INITIAL_ADMIN_PASSWORD}
-    ulimits:
-      memlock:
-        soft: -1 # Set memlock to unlimited (no soft or hard limit)
-        hard: -1
-      nofile:
-        soft: 65536 # Maximum number of open files for the opensearch user - set to at least 65536
-        hard: 65536
-    volumes:
-      # Creates volume called opensearch-data and mounts it to the container
-      - opensearch-data:/usr/share/opensearch/data
-    ports:
-      - 9200:9200 # REST API
-      - 9600:9600 # Performance Analyzer
   gotenberg:
     image: gotenberg/gotenberg:8
     container_name: gotenberg
@@ -100,6 +64,3 @@ services:
       - '50051:50051'
     volumes:
       - './spice/spicepod.yml:/app/spicepod.yaml:ro'
-
-volumes:
-  opensearch-data:
diff --git a/docs/disaster.md b/docs/disaster.md
index de45b1de0..0bae76ec4 100644
--- a/docs/disaster.md
+++ b/docs/disaster.md
@@ -11,7 +11,6 @@ Oh no! Here's how to recover from total data corruption, cyber-attack or regiona
 1. If this was a regional catastrophe, also update `awsRegion` to our secondary region (`us-east-2`)
 1. Run `npm run cdk:deploy-production` to deploy (may take up to an hour)
 1. Confirm that `app.infilla.com` is working (except for search-backed features)
-1. Sign into the AWS Lambda Console & trigger `reindex` for the `opensearch-command-runner` lambda
 
 If this was a regional catatastrophe, you also need replace any instances of `us-west-2` with `us-east-2` in `cdk.json` and use the `us-east-2` region for the AWS Web Console.
 
diff --git a/docs/environment.md b/docs/environment.md
index 4890440f4..69a5fdda3 100644
--- a/docs/environment.md
+++ b/docs/environment.md
@@ -77,22 +77,10 @@ STORAGE_SECRET=test
 STORAGE_REGION=us-east-2
 ```
 
-## Search
-
-We run OpenSearch to serve fulltext search queries.
-
 ### AWS deployment
 
 TODO: connection string
 
-### Local development
-
-In `.env`:
-
-```sh
-OPENSEARCH_URL="http://localhost:9200"
-```
-
 ## Hosting
 
 `APP_DOMAIN`: Public-facing domain name when deployed to AWS (defaults to `localhost`)
diff --git a/opensearch/migration.ts b/opensearch/migration.ts
deleted file mode 100755
index dee219659..000000000
--- a/opensearch/migration.ts
+++ /dev/null
@@ -1,41 +0,0 @@
-#! npx tsx
-/* eslint-disable no-console */
-
-import { getSearchClient } from '../app/services/search.server';
-
-import { type IndexSchema, ResponseError } from './types';
-
-export const configureIndex = async (
-  // eslint-disable-next-line @typescript-eslint/no-explicit-any
-  schema: IndexSchema<any, any>,
-) => {
-  const searchClient = await getSearchClient();
-  try {
-    console.info(`Creating index: ${schema.name}`);
-    await searchClient.indices.create({
-      index: schema.name,
-      body: { settings: schema.settings },
-    });
-    console.info(`Created index: ${schema.name}`);
-  } catch (e) {
-    if ((e as Error).name != 'ResponseError') {
-      throw e;
-    }
-    const error: ResponseError = e as ResponseError;
-
-    if (error.meta.body.error?.type == 'resource_already_exists_exception') {
-      // Note that you can't update settings for an existing index
-      // (create a new one and migrate over instead)
-      console.info('Index already exists. Skipping...');
-    } else {
-      throw e;
-    }
-  }
-
-  console.info(`Configuring mappings: ${schema.name}`);
-  await searchClient.indices.putMapping({
-    index: schema.name,
-    body: schema.mapping,
-  });
-  console.info(`Configured mapping: ${schema.name}`);
-};
diff --git a/opensearch/schema.ts b/opensearch/schema.ts
deleted file mode 100644
index a7634cc2b..000000000
--- a/opensearch/schema.ts
+++ /dev/null
@@ -1,129 +0,0 @@
-import { type Prisma } from '@prisma/client';
-
-import { formatRegulationId } from '../app/utils/regulation';
-import { getNode } from '../data/regulations';
-import { getTopicTag } from '../data/topic-tags';
-
-import { type IndexSchema } from './types';
-
-export const threadIndexSchema: IndexSchema<
-  Prisma.ThreadGetPayload<{
-    include: {
-      comments: true;
-      topicTags: true;
-      regulationTags: true;
-      organization: true;
-      originalComment: true;
-    };
-  }>,
-  {
-    id: string;
-    organizationId: string;
-    subject: string;
-    status: string;
-    isPublished: boolean;
-    commentsBodyText: string;
-    propertyAddress: string;
-    // TODO: Rename to "regulationIds"
-    legalCodeIds: string[];
-    legalCodeTitles: string[];
-    legalCodeShortTitles: string[];
-    topicTagSlugs: string[];
-    topicTagNames: string[];
-    updatedAt: Date;
-    createdAt: Date;
-    threadAuthorId: string;
-    viaChannel: string;
-  }
-> = {
-  name: 'thread',
-  settings: {},
-  mapping: {
-    dynamic: false,
-    properties: {
-      id: { type: 'keyword' },
-      organizationId: { type: 'keyword' },
-      status: { type: 'keyword' },
-      threadAuthorId: { type: 'keyword' },
-      viaChannel: { type: 'keyword' },
-      isPublished: { type: 'boolean' },
-      subject: {
-        type: 'text',
-        fields: {
-          english: { type: 'text', analyzer: 'english' },
-          suggestions: { type: 'search_as_you_type' },
-        },
-      },
-      commentsBodyText: {
-        type: 'text',
-        fields: {
-          english: { type: 'text', analyzer: 'english' },
-        },
-      },
-      propertyAddress: {
-        type: 'text',
-        fields: {
-          english: { type: 'text', analyzer: 'english' },
-          suggestions: { type: 'search_as_you_type' },
-        },
-      },
-      legalCodeIds: { type: 'keyword' },
-      legalCodeTitles: {
-        type: 'text',
-        fields: {
-          english: { type: 'text', analyzer: 'english' },
-        },
-      },
-      legalCodeShortTitles: { type: 'text' },
-      topicTagSlugs: { type: 'keyword' },
-      topicTagNames: {
-        type: 'text',
-        fields: {
-          english: { type: 'text', analyzer: 'english' },
-        },
-      },
-      updatedAt: { type: 'date' },
-      createdAt: { type: 'date' },
-    },
-  },
-  transform: (thread) => {
-    if (thread.deletedAt) {
-      return null;
-    }
-
-    return {
-      id: thread.id,
-      organizationId: thread.organizationId,
-      subject: thread.subject,
-      status: thread.status,
-      isPublished: thread.isPublished,
-      commentsBodyText: thread.comments.map((c) => c.bodyText).join('\n'),
-      propertyAddress: thread.propertyAddress || '',
-      topicTagSlugs: thread.topicTags.map((tag) => tag.topicTagSlug),
-      topicTagNames: thread.topicTags.map(
-        (tag) =>
-          getTopicTag(thread.organization.jurisdictionPlaceId, tag.topicTagSlug)?.name || 'Unknown',
-      ),
-      legalCodeIds: thread.regulationTags.map((tag) =>
-        formatRegulationId({
-          jurisdictionId: tag.jurisdictionId,
-          nodeId: tag.nodeId,
-        }),
-      ),
-      legalCodeTitles: thread.regulationTags.map((tag) => {
-        return getNode(tag.jurisdictionId, tag.nodeId).title;
-      }),
-      legalCodeShortTitles: thread.regulationTags.map((tag) => {
-        return getNode(tag.jurisdictionId, tag.nodeId).shortTitle;
-      }),
-      updatedAt: thread.updatedAt,
-      createdAt: thread.createdAt,
-
-      // TODO: Original comment is marked as optional on the data model. We should
-      // update this to not to default to empty string once we've enforced this
-      // on the data model
-      threadAuthorId: thread.originalComment?.authorId || '',
-      viaChannel: thread.originalComment?.viaChannel || 'agency',
-    };
-  },
-};
diff --git a/opensearch/types.ts b/opensearch/types.ts
deleted file mode 100644
index c1d39c043..000000000
--- a/opensearch/types.ts
+++ /dev/null
@@ -1,86 +0,0 @@
-// Error interface from @opensearch-project/opensearch
-export interface ResponseError {
-  name: 'ResponseError';
-  meta: {
-    body: {
-      error?:
-        | {
-            type: undefined; // needed for discriminated union typing
-            Message: string;
-          }
-        | {
-            type: string;
-            reason: string;
-            index: string;
-            index_uuid: string;
-          };
-      status: number;
-    };
-    statusCode: number;
-  };
-}
-
-export interface IndexSchema<
-  PrismaRecordT extends { id: string },
-  DocumentT extends { id: string },
-> {
-  name: string;
-  /** Index settings */
-  settings: { analysis?: AnalysisConfig };
-  /** Mapping settings */
-  mapping: {
-    // We never want to allow dynamic mappings (should always be explicit)
-    dynamic: false;
-    properties: Record<keyof DocumentT, PropertyMappingConfig>;
-  };
-  /** transform the input record into a document in the index, or null if it should be deleted */
-  transform: (record: PrismaRecordT) => DocumentT | null;
-}
-
-interface PropertyMappingConfig {
-  type: 'text' | 'keyword' | 'boolean' | 'date' | 'search_as_you_type';
-  fields?: Record<string, PropertyMappingConfig>;
-  analyzer?: string;
-}
-
-type AnalysisFilterConfig =
-  | {
-      type: 'stop';
-      stopwords: string;
-    }
-  | {
-      type: 'keyword_marker';
-      string: string[];
-    }
-  | {
-      type: 'stemmer';
-      language: string;
-    }
-  | {
-      type: 'stemmer_override';
-      rules: string[];
-    };
-
-interface AnalyzerConfig {
-  tokenizer: 'standard';
-  filter: string[];
-}
-
-interface AnalysisConfig {
-  filter: Record<string, AnalysisFilterConfig>;
-  analyzer: Record<string, AnalyzerConfig>;
-}
-
-export interface SearchHitsResult {
-  total: { value: number; relation: string };
-  max_score: number;
-  hits: SearchHitItem[];
-}
-
-/** Item in a list of `hits` from a search query */
-interface SearchHitItem {
-  _index: string;
-  _id: string;
-  _score: number;
-  _source: Record<string, number | number[] | boolean[] | boolean[] | object | object[] | null>;
-}
diff --git a/package-lock.json b/package-lock.json
index 4622e4ca8..302ff60cd 100644
--- a/package-lock.json
+++ b/package-lock.json
@@ -19,7 +19,6 @@
         "@hey-api/client-fetch": "^0.2.4",
         "@khanacademy/simple-markdown": "^2.0.9",
         "@microsoft/microsoft-graph-client": "^3.0.7",
-        "@opensearch-project/opensearch": "^2.8.0",
         "@posthog/react": "^1.3.0",
         "@prisma/client": "^6.3.1",
         "@radix-ui/react-accordion": "^1.2.1",
@@ -6735,29 +6734,6 @@
       "integrity": "sha512-XuySG1E38YScSJoMlqovLru4KTUNSjgVTIjyh7qMX6aNN5HY5Ct5LhRJdxO79JtTzKfzV/bnWpz+zquYrISsvw==",
       "license": "MIT"
     },
-    "node_modules/@opensearch-project/opensearch": {
-      "version": "2.8.0",
-      "resolved": "https://registry.npmjs.org/@opensearch-project/opensearch/-/opensearch-2.8.0.tgz",
-      "integrity": "sha512-VgYq5WQCHHotUlk8fUXAkisCjeiw6eNKg/z9elBHuFnNVEUEhSzl3UAuPS3Y9TUd9uTvUbAZ8kkuA5y3Te9rsw==",
-      "license": "Apache-2.0",
-      "dependencies": {
-        "aws4": "^1.11.0",
-        "debug": "^4.3.1",
-        "hpagent": "^1.2.0",
-        "ms": "^2.1.3",
-        "secure-json-parse": "^2.4.0"
-      },
-      "engines": {
-        "node": ">=10",
-        "yarn": "^1.22.10"
-      }
-    },
-    "node_modules/@opensearch-project/opensearch/node_modules/ms": {
-      "version": "2.1.3",
-      "resolved": "https://registry.npmjs.org/ms/-/ms-2.1.3.tgz",
-      "integrity": "sha512-6FlzubTLZG3J2a/NVCAleEhjzq5oxgHyaCU9yYXvcLsvoVaHJq/s5xXI6/XXP6tz7R9xAOtHnSO/tXtF3WRTlA==",
-      "license": "MIT"
-    },
     "node_modules/@opentelemetry/api": {
       "version": "1.8.0",
       "resolved": "https://registry.npmjs.org/@opentelemetry/api/-/api-1.8.0.tgz",
@@ -22276,12 +22252,6 @@
         "node": ">=10"
       }
     },
-    "node_modules/aws4": {
-      "version": "1.13.0",
-      "resolved": "https://registry.npmjs.org/aws4/-/aws4-1.13.0.tgz",
-      "integrity": "sha512-3AungXC4I8kKsS9PuS4JH2nc+0bVY/mjgrephHTIi8fpEeGsTHBUJeosp0Wc1myYMElmD0B3Oc4XL/HVJ4PV2g==",
-      "license": "MIT"
-    },
     "node_modules/axe-core": {
       "version": "4.10.3",
       "resolved": "https://registry.npmjs.org/axe-core/-/axe-core-4.10.3.tgz",
@@ -27569,15 +27539,6 @@
         "node": ">=12"
       }
     },
-    "node_modules/hpagent": {
-      "version": "1.2.0",
-      "resolved": "https://registry.npmjs.org/hpagent/-/hpagent-1.2.0.tgz",
-      "integrity": "sha512-A91dYTeIB6NoXG+PxTQpCCDDnfHsW9kc06Lvpu1TEe9gnd6ZFeiBoRO9JvzEv6xK7EX97/dUE8g/vBMTqTS3CA==",
-      "license": "MIT",
-      "engines": {
-        "node": ">=14"
-      }
-    },
     "node_modules/html-encoding-sniffer": {
       "version": "4.0.0",
       "resolved": "https://registry.npmjs.org/html-encoding-sniffer/-/html-encoding-sniffer-4.0.0.tgz",
@@ -38682,12 +38643,6 @@
         "node": ">=16"
       }
     },
-    "node_modules/secure-json-parse": {
-      "version": "2.7.0",
-      "resolved": "https://registry.npmjs.org/secure-json-parse/-/secure-json-parse-2.7.0.tgz",
-      "integrity": "sha512-6aU+Rwsezw7VR8/nyvKTx8QpWH9FrcYiXXlqC4z5d5XQBDRqtbfsRjnwGyqbi3gddNtWHuEk9OANUotL26qKUw==",
-      "license": "BSD-3-Clause"
-    },
     "node_modules/selderee": {
       "version": "0.11.0",
       "resolved": "https://registry.npmjs.org/selderee/-/selderee-0.11.0.tgz",
diff --git a/package.json b/package.json
index 02e8fccfb..d605b4486 100644
--- a/package.json
+++ b/package.json
@@ -32,8 +32,6 @@
     "cdk:deploy": "aws sts get-caller-identity --profile development && npm run pre-deploy '_dev_' && SENTRY_RELEASE=$(git rev-parse --short HEAD) MODE=staging dotenv -e .env.staging -- cdk --profile development -c config=development deploy --all && npm run cdk:cleanup",
     "cdk:diff-production": "aws sts get-caller-identity --profile production cdk --profile production -c config=production diff",
     "cdk:deploy-production": "aws sts get-caller-identity --profile production && npm run pre-deploy '*prod*' && SENTRY_RELEASE=$(git rev-parse --short HEAD) MODE=production dotenv -e .env.production -- cdk --profile production -c config=production deploy --all  && npm run cdk:cleanup",
-    "opensearch:migrate": "tsx --env-file=.env.development ./scripts/opensearch.ts migrate",
-    "opensearch:reindex": "tsx --env-file=.env.development ./scripts/opensearch.ts reindex",
     "pre-deploy": "tsx --env-file=.env.development ./scripts/deploy/pre-deploy.ts",
     "prisma:generate": "prisma generate",
     "prisma:migrate:local": "dotenv -e .env.development -- npx prisma migrate dev",
@@ -133,7 +131,6 @@
     "@hey-api/client-fetch": "^0.2.4",
     "@khanacademy/simple-markdown": "^2.0.9",
     "@microsoft/microsoft-graph-client": "^3.0.7",
-    "@opensearch-project/opensearch": "^2.8.0",
     "@posthog/react": "^1.3.0",
     "@prisma/client": "^6.3.1",
     "@radix-ui/react-accordion": "^1.2.1",
diff --git a/scripts/forum/import-questions.ts b/scripts/forum/import-questions.ts
index c14208b4b..102bf22b1 100644
--- a/scripts/forum/import-questions.ts
+++ b/scripts/forum/import-questions.ts
@@ -199,7 +199,7 @@ async function importQuestions(path: string, organizationSlug: string) {
     });
   }
 
-  // Re-index all threads for the organization in opensearch
+  // Re-index all threads for the organization
   await indexManyThreads({
     where: {
       organizationId: organization.id,
diff --git a/scripts/opensearch.ts b/scripts/opensearch.ts
deleted file mode 100755
index 1811ca4be..000000000
--- a/scripts/opensearch.ts
+++ /dev/null
@@ -1,84 +0,0 @@
-#! npx tsx
-import commandLineArgs from 'command-line-args';
-import commandLineUsage from 'command-line-usage';
-
-import logger from '../app/services/logger.server';
-
-import { runMigrate, runReindex } from './opensearch/commands';
-
-const CLI_OPTION_DEFINITIONS: commandLineUsage.OptionDefinition[] = [
-  {
-    name: 'command',
-    description: 'command to perform',
-    type: String,
-    defaultOption: true,
-    typeLabel: '<command>',
-  },
-  {
-    name: 'help',
-    description: 'Show usage information',
-    type: Boolean,
-    alias: 'h',
-  },
-];
-
-const COMMAND_OPTIONS = [
-  { name: 'migrate', description: 'configure all indices and mappings' },
-  { name: 'reindex', description: 'reindex all indices' },
-];
-
-const CLI_USAGE_SECTIONS: commandLineUsage.Section[] = [
-  {
-    header: 'OpenSearch',
-    content: 'Manage OpenSearch configuration and documents',
-  },
-  {
-    header: 'Usage',
-    content: ['$ ./scripts/opensearch-migrate.ts <options> <command>'],
-  },
-  {
-    header: 'Command list',
-    content: COMMAND_OPTIONS.map(({ name, description }) => {
-      return { name, summary: description };
-    }),
-  },
-  {
-    header: 'Options',
-    optionList: CLI_OPTION_DEFINITIONS.filter((option) => option.name != 'command'),
-  },
-  {
-    header: 'Examples',
-    content: [
-      {
-        desc: 'Update all indices to the current schema',
-        example: '$ ./scripts/opensearch.ts migrate',
-      },
-      {
-        desc: 'Reindex all documents',
-        example: '$ ./scripts/opensearch.ts reindex',
-      },
-    ],
-  },
-];
-
-const cliOptions = commandLineArgs(CLI_OPTION_DEFINITIONS) as {
-  command?: string;
-  help?: boolean;
-};
-
-if (cliOptions.help) {
-  logger.info(commandLineUsage(CLI_USAGE_SECTIONS));
-} else if (cliOptions.command) {
-  switch (cliOptions.command) {
-    case 'migrate':
-      runMigrate();
-      break;
-    case 'reindex':
-      runReindex();
-      break;
-    default:
-      logger.error(`Unrecognized command ${cliOptions.command}`);
-  }
-} else {
-  logger.info(commandLineUsage(CLI_USAGE_SECTIONS));
-}
diff --git a/scripts/opensearch/commands.ts b/scripts/opensearch/commands.ts
deleted file mode 100644
index 125887139..000000000
--- a/scripts/opensearch/commands.ts
+++ /dev/null
@@ -1,12 +0,0 @@
-import { indexManyThreads } from '../../app/search/indexers/thread.server';
-import logger from '../../app/services/logger.server';
-import { configureIndex } from '../../opensearch/migration';
-import { threadIndexSchema } from '../../opensearch/schema';
-export const runMigrate = async () => {
-  await configureIndex(threadIndexSchema);
-};
-
-export const runReindex = async () => {
-  await indexManyThreads({ where: {} }) // all records
-    .then((indexedCount) => logger.info(`Indexed ${indexedCount} threads`));
-};
diff --git a/server-lambda.js b/server-lambda.js
index b9db72d6c..83bb4c9ec 100644
--- a/server-lambda.js
+++ b/server-lambda.js
@@ -3,21 +3,12 @@ import { createRequestHandler } from '@remix-run/architect';
 
 import { connectPrisma, getPrisma } from './app/services/db.server';
 import { posthogClient } from './app/services/posthog.server';
-import { getSearchClient } from './app/services/search.server';
 import { getAuthSecret } from './app/services/secrets/auth-secret.server';
 import * as build from './build/server/index.js';
 
 // Make initial requests for secrets and service connections so they're warmed before
 // handling invocation requests (can add multiple seconds of latency)
-const [prisma, _authSecret, _searchClient] = await Promise.all([
-  getPrisma(),
-  getAuthSecret(),
-  getSearchClient().then(async (client) => {
-    // No-op request (just enough to warm DNS caches)
-    await client.cluster.health();
-    return client;
-  }),
-]);
+const [prisma, _authSecret] = await Promise.all([getPrisma(), getAuthSecret()]);
 
 const handleRequest = createRequestHandler({
   build: build,
